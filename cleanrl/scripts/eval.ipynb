{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tyro\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "oc_atari_dir = os.getenv(\"OC_ATARI_DIR\")\n",
    "\n",
    "if oc_atari_dir is not None:\n",
    "    a = os.path.join(Path(__file__), oc_atari_dir)\n",
    "    sys.path.insert(1, a)\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "has_agent= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"ALE/Pong-v5\"\n",
    "obs_mode = \"masked_dqn\"\n",
    "pth = \"/data/oc_cleanrl/cleanrl/wandb/run-20241010_060604-pzk9e36i/files/DQNLv2.cleanrl_model\"\n",
    "architecture = \"DQNLv2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OBJ State Representation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "from ocatari.core import OCAtari\n",
    "env = OCAtari(\n",
    "    env_id, hud=False, render_mode=\"rgb_array\",\n",
    "        render_oc_overlay=False, obs_mode=obs_mode,\n",
    "        # logger=logger, feature_func=feature_func,\n",
    "        # buffer_window_size=window_size\n",
    ")\n",
    "\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if architecture == \"OCT\":\n",
    "    from architectures.transformer import OCTransformer as Agent\n",
    "    agent = Agent(env, emb_dim, num_heads, num_blocks, device).to(device)\n",
    "elif architecture == \"VIT\":\n",
    "    from architectures.transformer import VIT as Agent\n",
    "    agent = Agent(env, emb_dim, num_heads, num_blocks,\n",
    "                    patch_size, buffer_window_size, device).to(device)\n",
    "elif architecture == \"VIT2\":\n",
    "    from architectures.transformer import SimpleViT2 as Agent\n",
    "    agent = Agent(env, emb_dim, num_heads, num_blocks,\n",
    "                    patch_size, buffer_window_size, device).to(device)\n",
    "elif architecture == \"MobileVit\":\n",
    "    from architectures.transformer import MobileVIT as Agent\n",
    "    agent = Agent(env, emb_dim, num_heads, num_blocks,\n",
    "                    patch_size, buffer_window_size, device).to(device)\n",
    "elif architecture == \"MobileVit2\":\n",
    "    from architectures.transformer import MobileViT2 as Agent\n",
    "    agent = Agent(env, emb_dim, num_heads, num_blocks,\n",
    "                    patch_size, buffer_window_size, device).to(device)\n",
    "elif architecture == \"PPO\":\n",
    "    from architectures.ppo import PPODefault as Agent\n",
    "    agent = Agent(env, device).to(device)\n",
    "else:\n",
    "    from architectures.ppo import PPO_Obj as Agent\n",
    "    agent = Agent(env, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(pth, map_location=torch.device('cpu'))\n",
    "agent.load_state_dict(ckpt[\"model_weights\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 84, 84])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = torch.from_numpy(obs).to(device)\n",
    "obs = obs.unsqueeze(0)\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def print_obs(obs):\n",
    "    \n",
    "    # Display the array as an image\n",
    "    plt.imshow(obs, cmap='gray')  # Use 'gray' colormap for 2D arrays\n",
    "    plt.axis('off')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALE/Pong-v5 (H): Reward is episode 0 is -20.0 Length is episode 0 is 4445\n",
      "ALE/Pong-v5 (H): Reward is episode 1 is -21.0 Length is episode 1 is 3764\n",
      "ALE/Pong-v5 (H): Reward is episode 2 is -21.0 Length is episode 2 is 3056\n",
      "ALE/Pong-v5 (H): Reward is episode 3 is -20.0 Length is episode 3 is 4434\n",
      "ALE/Pong-v5 (H): Reward is episode 4 is -20.0 Length is episode 4 is 3675\n",
      "ALE/Pong-v5 (H): Reward is episode 5 is -17.0 Length is episode 5 is 5318\n",
      "ALE/Pong-v5 (H): Reward is episode 6 is -21.0 Length is episode 6 is 3280\n",
      "ALE/Pong-v5 (H): Reward is episode 7 is -21.0 Length is episode 7 is 3896\n",
      "ALE/Pong-v5 (H): Reward is episode 8 is -21.0 Length is episode 8 is 4606\n",
      "ALE/Pong-v5 (H): Reward is episode 9 is -17.0 Length is episode 9 is 5570\n",
      "ALE/Pong-v5 (H): Reward is episode 10 is -21.0 Length is episode 10 is 3766\n",
      "ALE/Pong-v5 (H): Reward is episode 11 is -21.0 Length is episode 11 is 3304\n",
      "ALE/Pong-v5 (H): Reward is episode 12 is -21.0 Length is episode 12 is 3900\n",
      "ALE/Pong-v5 (H): Reward is episode 13 is -21.0 Length is episode 13 is 3760\n",
      "ALE/Pong-v5 (H): Reward is episode 14 is -21.0 Length is episode 14 is 3304\n",
      "ALE/Pong-v5 (H): Reward is episode 15 is -20.0 Length is episode 15 is 3479\n",
      "ALE/Pong-v5 (H): Reward is episode 16 is -19.0 Length is episode 16 is 3973\n",
      "ALE/Pong-v5 (H): Reward is episode 17 is -21.0 Length is episode 17 is 3056\n",
      "ALE/Pong-v5 (H): Reward is episode 18 is -20.0 Length is episode 18 is 3849\n",
      "ALE/Pong-v5 (H): Reward is episode 19 is -20.0 Length is episode 19 is 3969\n",
      "ALE/Pong-v5 (H): Reward is episode 20 is -21.0 Length is episode 20 is 3243\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    done = False\n",
    "    crew = 0\n",
    "    while not done:\n",
    "        #action, _, _, _ = agent.get_action_and_value(obs)\n",
    "        action = env.action_space.sample() # random moves\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        #print_obs(obs[0])\n",
    "\n",
    "        obs = torch.from_numpy(obs).to(device)\n",
    "        obs = obs.unsqueeze(0)\n",
    "        crew += reward\n",
    "\n",
    "        \n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"{env_id} (H): Reward is episode {i} is\", crew, f\"Length is episode {i} is\",info[\"episode_frame_number\"])\n",
    "            #run.log({f\"{opts.game}_reward\": crew, f\"{opts.game}_episode_length\": info[\"episode_frame_number\"]})\n",
    "            observation, info = env.reset()\n",
    "            done = True\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
